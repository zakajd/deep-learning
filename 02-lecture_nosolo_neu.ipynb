{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Персептрон и алгоритм обратного распространения ошибки.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustration of the Mark 1 perceptron hardware (1960).\n",
    "\n",
    "<img src=\"./imgs/0.png\">\n",
    "The photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene, in this case a printed character, was illuminated by powerful lights, and an image focussed onto a 20 × 20 array of cadmium sulphide photocells, giving a primitive 400 pixel image. \n",
    "<img src=\"./imgs/1.png\">\n",
    "The perceptron also had a patch board, shown in the middle photograph,\n",
    "which allowed different configurations of input features to be tried. Often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern digital computer. \n",
    "\n",
    "<img src=\"./imgs/2.png\">\n",
    "The photograph on the right shows one of the racks of adaptive weights. Each weight was\n",
    "implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby\n",
    "allowing the value of the weight to be adjusted automatically by the learning algorithm.\n",
    "\n",
    "<img src=\"./imgs/2.jpg\">\n",
    "The Mark 1 Perceptron (Source: Arvin Calspan Advanced Technology Center; Hecht-Nielsen, R. Neurocomputing (Reading, Mass.: Addison-Wesley, 1990).)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Многослойный перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/3.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large\n",
    "\\large OUT(x) = softmax(D_2(\\vec{x})) \\\\\n",
    "\\large D_2(\\vec{x}) = \\theta_2 + W_2 A_1(\\vec{x}) \\\\\n",
    "\\large A_1(x) = ReLU(0, D_1(\\vec{x})) \\\\\n",
    "\\large D_1(\\vec{x}) = \\theta_1 + W_1 \\vec{x}\n",
    "$$\n",
    "\n",
    "Каждая сеть представима в виде акцикличного графа вычислений над тензорами. **Тензор** - в случае нейронных сетей это просто многомерный массив. В узлах графа находятся функции, осуществляющие операции. В техминах нейронных сетей вычислительные узлы называют **слоями**. В данной сети есть слои следующих видов:\n",
    "- dense (D) - полносвязный слой\n",
    "- ReLU(0,x) - слой нелинейной активации\n",
    "- softmax - softmax \n",
    "- loss - функционал потерь, котороый тоже стоит рассматривать как слой.\n",
    "\n",
    "Обычно граф вычислений рисуется следующим образом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/4.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как мы говорим про задачу многоклассовой классификации, то на выходе сети мы ожидаем набор вероятностей $p_i$, такие что $\\sum p_j = 1$. Для реализации этого используется softmax слой, физический смысл которого эквивалентен сигмойду в задаче бинарной классификации\n",
    "$$\n",
    "\\large softmax_i = p_i = \\frac{e^{a_i}}{\\sum_j e^{a_j}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss функция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы получить loss функцию для нейронной сети, рассмотрим понятие перекрестной энтропии:\n",
    "$$\n",
    "\\large H(p, q) = \\operatorname{E}_p[-\\log q] = H(p) + D_{\\mathrm{KL}}(p \\| q)=-\\sum_x p(x)\\log q(x)\n",
    "$$\n",
    "где $D_{\\mathrm{KL}}(p||q)$ - расстояние Ку́льбака—Ле́йблера\n",
    "$$\n",
    "\\large D_{\\mathrm{KL}}(p||q)=\\sum \\limits _{i=1}^{n}p_{i}\\log {\\frac {p_{i}}{q_{i}}}\n",
    "$$\n",
    "Значение этого функционала можно понимать как количество неучтённой информации распределения $P$, если $Q$ было использовано для приближения $P$.\n",
    "\n",
    "Можно показать, что оптимизация кросс-энтропии в задаче многоклассовой оптимизации эквивалентно оптимизации правдоподобия.  Таким образом, loss для нашей сети будет:\n",
    "$$\n",
    "\\large L = -\\sum_i y_i \\log \\hat{y}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Концепция автоматического дифференцирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте переместимся в слайды )\n",
    "<img src=\"./imgs/5.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучаем персептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: нужно реализовать двухслойный персептрон и обучить его на наборе MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полносвязный слой (Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_size, out_size, rlambda = 0.0):\n",
    "        self.W = np.random.normal(scale=1, size=(out_size, in_size)) * np.sqrt(2 / in_size)\n",
    "        self.b = np.zeros(out_size)\n",
    "        self.rlambda = rlambda\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x # запоминаем для обратного прохода\n",
    "        return np.dot(self.W, x) + self.b\n",
    "    \n",
    "    def get_reg_loss(self):\n",
    "        return 0.5 * self.rlambda * (np.linalg.norm(self.W, ord='fro') ** 2)\n",
    "    \n",
    "    def backward(self, dz, lr=0.001):\n",
    "        # вычисляем градиенты по параметрам (запоминаем их для отладки)\n",
    "        self.dW = np.outer(dz, self.x)\n",
    "        self.db = dz\n",
    "        # вычисляем производную по входу\n",
    "        self.dx = np.matmul(dz, self.W) \n",
    "        \n",
    "        # рассчитываем градиенты от регуляризатора\n",
    "        if(self.rlambda != 0):\n",
    "            self.dW += self.rlambda * self.W\n",
    "        # обновляем веса\n",
    "        self.W = self.W - lr * self.dW\n",
    "        self.b = self.b - lr * self.db\n",
    "        # возвращаем dx для продолжения алгоритма\n",
    "        return self.dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем используя пример из слайдов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  [1.22 0.76] must be [1.22 0.76] diff =  [0. 0.]\n",
      "dx =  [-0.112  0.636] must be [-0.112  0.636] diff =  [1.38777878e-17 0.00000000e+00]\n",
      "W\n",
      " [[ 0.099912  0.499824]\n",
      " [-0.300104  0.799792]]\n",
      "dW:\n",
      " [[0.088 0.176]\n",
      " [0.104 0.208]]\n",
      "db:\n",
      " [0.44 0.52]\n",
      "dx:\n",
      " [-0.112  0.636]\n"
     ]
    }
   ],
   "source": [
    "d = Dense(2, 2)\n",
    "x = np.array([0.2, 0.4])\n",
    "d.W = np.array([[0.1, 0.5], [-0.3, 0.8]])\n",
    "d.b = np.array([1, 0.5])\n",
    "dz = np.array([0.44, 0.52])\n",
    "\n",
    "z = d.forward(x)\n",
    "z_orig = np.array([1.22, 0.76])\n",
    "print('z = ',z, 'must be', z_orig, 'diff = ', z-z_orig)\n",
    "\n",
    "dx = d.backward(dz)\n",
    "dx_orig = np.array([-0.112, 0.636])\n",
    "print('dx = ', dx, 'must be', dx_orig, 'diff = ', dx-dx_orig)\n",
    "\n",
    "print('W\\n', d.W)\n",
    "print('dW:\\n', d.dW)\n",
    "print('db:\\n', d.db)\n",
    "print('dx:\\n', d.dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В слайдах у нас вход и выход слоя имеют одинаковую размерность. Давайте проверим, что для разных размеров все считается правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) must be (2,)\n",
      "(3,) must be (3,)\n"
     ]
    }
   ],
   "source": [
    "# Проверяем, что размерности соблюдаются правильно\n",
    "d = Dense(3, 2)\n",
    "x = np.array([0.1, 0.2, 0.3])\n",
    "r = d.forward(x)\n",
    "print(r.shape, 'must be', (2,))\n",
    "dz = np.array([0.4, 0.5])\n",
    "dz = d.backward(dz)\n",
    "print(dz.shape, 'must be', (3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нелинейная функция активации: ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLu = max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dz, lr=0.1):\n",
    "        dz[self.x < 0] = 0\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial }{\\partial x_{k}}\\sigma_i (x) =\\sigma_i (x)(\\delta _{ik}-\\sigma_k (x))\n",
    "$$\n",
    "где $\\delta _{ik}$ это дельта Кронекера\n",
    "$$\n",
    "\\delta _{{ij}}={\\begin{cases}0&{\\text{if }}i\\neq j,\\\\1&{\\text{if }}i=j.\\end{cases}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object): \n",
    "    \"\"\"Vanilla Softmax layer for Neural Network\"\"\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # a^(L) activitation vector from L layer\n",
    "        # C used to prevent exponents \"explosion\"\n",
    "        self.X = X\n",
    "        # Find max for each column\n",
    "        C = - np.max(X, axis=0)\n",
    "        self.exps = np.exp(X + C)\n",
    "        #sum of elements in each column == 1\n",
    "        return self.exps / np.sum(self.exps, axis=0)\n",
    "        \n",
    "    \n",
    "    def backward(self, dz):\n",
    "        frwd = self.exps / np.sum(self.exps, axis=0)\n",
    "        #matrix = np.matmul(a, np.ones((1, 3))) * (np.identity(3) - np.matmul(np.ones((3, 1)), a.T))\n",
    "        print(frwd)\n",
    "        length = frwd.shape[0]\n",
    "        #length = len(frwd)\n",
    "        dx = np.identity(length)\n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                dx[i, j] = frwd[i,i]* (dx[i,j] - frwd[j,i])\n",
    "        return dx * dz # rows contain all partial derivatives for i-th component of frwd\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01798621 0.11920292]\n",
      " [0.98201379 0.88079708]]\n",
      "[[0.01798621 0.11920292]\n",
      " [0.98201379 0.88079708]]\n",
      "[[ 0.         -0.        ]\n",
      " [ 0.09449423 -0.09449423]]\n"
     ]
    }
   ],
   "source": [
    "# Код для проверки\n",
    "#x = np.array([0.1, -0.1])\n",
    "x = np.array([[1, 4], [5, 6]])\n",
    "s = Softmax()\n",
    "sm = s.forward(x)\n",
    "print(sm)\n",
    "\n",
    "dz = np.array([[0, 0], [-0.9, -0.9]])\n",
    "print(s.backward(dz))\n",
    "\n",
    "#print(s.lp)\n",
    "#print(s.lp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss: кросс-энтропия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомню, кросс-энтропия выражается как\n",
    "$$\n",
    "L = -\\sum_i y_i \\log \\hat{y}_i\n",
    "$$\n",
    "\n",
    "Градиент вычисляется следующим образом:\n",
    "$$\n",
    "\\frac {\\partial }{\\partial \\hat{y}_{i}} L = -\\frac{y_i}{\\hat{y}_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "class CrossEntropy:\n",
    "    \n",
    "    def forward(self, y_true, y_hat):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dz, lr=0.001):\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация параметров сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике часто используют, так называемое, правило Kaiming He:\n",
    "$$\n",
    "W \\sim N(0,\\sqrt{\\frac{2}{n_{in}}})\n",
    "$$\n",
    "\n",
    "Почему так:\n",
    "$$\n",
    "Y = W_1X_1 + W_2X_2 + ... + W_nX_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(W_iX_i) = E[X_i]^2 \\text{Var}(W_i) + E[W_i]^2 \\text{Var}(X_i) + \\text{Var}(W_i)\\text{Var}(X_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(W_iX_i) = E[X_i^2] \\text{Var}(W_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(Y) = \\text{Var}(W_1X_1 + W_2X_2 + \\dotsb + W_n X_n) = n\\text{Var}(W)E[X^2]\n",
    "$$\n",
    "\n",
    "На *l-ом* слое:\n",
    "$$\n",
    "X^{l} = ReLU(0,Y^{l-1}) \\text{, а } Y^{l-1} \\text{распределен симметрично относительно нуля}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[(X^{l})^2] = \\frac{1}{2} \\text{Var}(Y^{l-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(Y^l) = \\frac{n}{2}\\text{Var}(W_i)\\text{Var}(Y^{l-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(W_i) = \\frac{2}{n} = \\frac{2}{n_\\text{in}} \n",
    "$$\n",
    "\n",
    "Для симметричных функций активации(сигмоид,тангенс) используем правило Xavier'а:\n",
    "\n",
    "$$\n",
    "W \\sim U(-\\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}},-\\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dropout (в дополнение к L1 и L2). \"Учиться меньше, чтобы учиться лучше\"(с)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/18.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть: каждый из нейронов участвует в обучении с некоторой вероятностью __p__. Во время инференса все нейроны включены в работу сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p = 0.5):\n",
    "        self.p = p\n",
    "        self.train = True\n",
    "    \n",
    "    def set_train(self, train = True):\n",
    "        self.train = train\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.train:\n",
    "            self.mask = np.ones(*x.shape)\n",
    "            return x\n",
    "        self.mask = ( np.random.rand(*x.shape) > self.p ) / (1.0 - self.p)\n",
    "        return x * self.mask\n",
    "        \n",
    "    def backward(self, dz, lr=0.001):\n",
    "        return dz * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed()\n",
    "x = np.array([1,2,3,4,5,6,7,8])\n",
    "d = Dropout(0.5)\n",
    "z = d.forward(x)\n",
    "print(z)\n",
    "z = d.backward(z)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест 0 - самая тупая сеть\n",
    "\n",
    "Строим функцию $f(x)=x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1., 0],\n",
    "              [0,  1.]])\n",
    "Y = np.array([[1., 0],\n",
    "              [0,  1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = Dense(2, 2)\n",
    "        self.s = Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        net = self.d.forward(x)\n",
    "        net = self.s.forward(net)\n",
    "        return net\n",
    "    \n",
    "    def backward(self, dz, lr=0.001):\n",
    "        dz = self.s.backward(dz, lr)\n",
    "        dz = self.d.backward(dz, lr)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = IdentityNet()\n",
    "L_iter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем loss, его удобнее хранить в теле обучения\n",
    "loss = CrossEntropy() \n",
    "lr = 0.1 # скорость обучения\n",
    "for iter in range(100): # эпохи = полный проход по датасету\n",
    "    L_acc = 0.\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        y_h = net.forward(x)\n",
    "        L = loss.forward(y, net.forward(x))\n",
    "        L_acc += L\n",
    "        dz = loss.backward(1, lr)\n",
    "        net.backward(dz, lr)\n",
    "    L_iter.append(L_acc) # коллекционируем loss\n",
    "plt.plot(L_iter) # посмотрим на кривую обучения\n",
    "print( net.forward(X[0]), net.forward(X[1])) # проверим глазами, что обучились"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест 1 - XOR\n",
    "\n",
    "<img src=\"./imgs/10.png\" width=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1],\n",
    "              [1, -1],\n",
    "              [-1, 1],\n",
    "              [1, 1]], dtype='float')\n",
    "\n",
    "Y = np.array([[1, 0],\n",
    "              [0, 1],\n",
    "              [0, 1],\n",
    "              [1, 0]], dtype='float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XorNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d1 = Dense(2, 10)\n",
    "        self.a1 = ReLU()\n",
    "        self.d2 = Dense(10, 2)\n",
    "        self.sm = Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        net = self.d1.forward(x)\n",
    "        net = self.a1.forward(net)\n",
    "        net = self.d2.forward(net)\n",
    "        net = self.sm.forward(net)\n",
    "        self.net = net\n",
    "        return net\n",
    "    \n",
    "    def backward(self, dz, lr=0.1):\n",
    "        dz = self.sm.backward(dz, lr)\n",
    "        dz = self.d2.backward(dz, lr)\n",
    "        dz = self.a1.backward(dz, lr)\n",
    "        dz = self.d1.backward(dz, lr)\n",
    "        return dz\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = XorNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropy()\n",
    "lr = 0.1\n",
    "L_iter = []\n",
    "for iter in range(100):\n",
    "    L_acc = 0.\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        y_h = net.forward(x)\n",
    "        L = loss.forward(y, y_h)\n",
    "        L_acc += L\n",
    "        dz = loss.backward(1, lr)\n",
    "        dz = net.backward(dz, lr)\n",
    "    L_iter.append(L_acc)\n",
    "plt.plot(L_iter)\n",
    "for i in range(4):\n",
    "    print(net.forward(X[i]), Y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест 3 - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home='./')\n",
    "X = mnist['data']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "for i in range(n*n):\n",
    "    plt.subplot(n,n,i+1)\n",
    "    I = X[np.random.randint(0, X.shape[0]),:]\n",
    "    I = I.reshape((28, 28))\n",
    "    plt.imshow(I, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float')\n",
    "Xm = np.mean(X, axis=0)\n",
    "Xs = np.std(X, axis=0)\n",
    "X=(X - Xm) / (Xs + 0.01)\n",
    "plt.imshow(Xm.reshape((28, 28)))\n",
    "plt.figure()\n",
    "plt.plot(X[0].reshape((28, 28))[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "Y = mnist['target']\n",
    "\n",
    "idxs = np.where((Y == 5 ) | (Y == 6))\n",
    "\n",
    "X_n = X[idxs]\n",
    "Y_n = Y[idxs]\n",
    "\n",
    "print('original', Y_n)\n",
    "t = OneHotEncoder(sparse=False)\n",
    "Y_work = t.fit_transform(Y_n.reshape(-1, 1))\n",
    "print('one hot', Y_work)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_n, Y_work, test_size=0.33, stratify=Y_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistNet:\n",
    "    \n",
    "    def __init__(self, rlambda=0.0):\n",
    "        self.d = Dense(784, 2, rlambda)\n",
    "        self.m = ReLU()\n",
    "        self.s = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        net = self.d.forward(x)\n",
    "        net = self.m.forward(net)\n",
    "        net = self.s.forward(net)\n",
    "        return net\n",
    "    \n",
    "    def backward(self, dz, lr):\n",
    "        dz = self.s.backward(dz, lr)\n",
    "        dz = self.m.backward(dz, lr)\n",
    "        dz = self.d.backward(dz, lr)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MnistNet(0)\n",
    "loss = CrossEntropy()\n",
    "lr = 0.001\n",
    "L_train = []\n",
    "L_test = []\n",
    "for iter in range(500):\n",
    "    L_acc = 0.\n",
    "    sh = list(range(X_train.shape[0])) # больше рандома богу рандома\n",
    "    np.random.shuffle(sh)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        x = X_train[sh[i]]\n",
    "        y = Y_train[sh[i]]\n",
    "        y_h = net.forward(x)\n",
    "        L = loss.forward(y, y_h) #+ net.get_reg_loss()\n",
    "        L_acc += L \n",
    "        dz = loss.backward(1, lr)\n",
    "        dz = net.backward(dz, lr)\n",
    "    L_acc /= Y_train.shape[0]\n",
    "    L_train.append(L_acc)\n",
    "    L_e_acc = 0.\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x = X_test[i]\n",
    "        y = Y_test[i]\n",
    "        y_h = net.forward(x)\n",
    "        L = loss.forward(y, y_h) #+ net.get_reg_loss()\n",
    "        L_e_acc += L\n",
    "    L_e_acc /= Y_test.shape[0]\n",
    "    L_test.append(L_e_acc)\n",
    "    if not iter % 25:\n",
    "        print(\"{} iter loss. Train : {} . Test : {}\".format(iter, L_acc, L_e_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L_train, label='train')\n",
    "plt.plot(L_test, label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "sh = list(range(X_test.shape[0]))\n",
    "np.random.shuffle(sh)\n",
    "\n",
    "n = 3\n",
    "for i in range(n*n):\n",
    "    ax = plt.subplot(n, n, i+1)\n",
    "\n",
    "    j = sh[i]\n",
    "    R = np.argmax(Y_test[j])\n",
    "    P = net.forward(X_test[j])\n",
    "    Rp = np.argmax(P)\n",
    "\n",
    "    ax.set_title('%s = %.2f R:%s' % (Rp, P[Rp], R))\n",
    "    ax.set_axis_off()\n",
    "    plt.imshow(X_test[j].reshape((28, 28)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "for i in range(0, 2):\n",
    "    ax = plt.subplot(1, 2, i + 1)\n",
    "    ax.set_title('%s' % (i))\n",
    "    ax.set_axis_off()\n",
    "    plt.imshow( net.d.W[i].reshape((28, 28)).T, cmap='gray' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch. Autograd.\n",
    "\n",
    "Переходим в другой ноутбук."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чеклист"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- устройство многослойного персептрона\n",
    "- слои: полносвязный, ReLu, Softmax, Dropout, CrossEntropy\n",
    "- chain rule\n",
    "- правило дифференцирования сложной функции\n",
    "- алгоритм обратного распространения ошибки\n",
    "- Xavier's / He's инициализации\n",
    "- почему при обучении сеть потребляет больше памяти, чем при выводе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Почитать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop: лекция в cs231n\n",
    "- почитать: http://cs231n.github.io/optimization-2/\n",
    "- посмотреть: https://www.youtube.com/watch?v=d14TUNcbn1k\n",
    "\n",
    "Устройство мозга\n",
    "- медицина: https://www.coursera.org/learn/medical-neuroscience\n",
    "- информатика: https://www.coursera.org/learn/computational-neuroscience\n",
    "\n",
    "Нейроморфные процессоры\n",
    "- IBM SyNAPSE chip: http://research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
