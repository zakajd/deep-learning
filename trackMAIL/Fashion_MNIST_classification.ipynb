{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST classification using PyTorch\n",
    "\n",
    "In this notebook we will try to classify the Fashion MNIST dataset\n",
    "(https://github.com/zalandoresearch/fashion-mnist) using VGG-like architectures (https://arxiv.org/abs/1409.1556). This notebook is inspired from the MNIST example from PyTorch (https://github.com/pytorch/examples/tree/master/mnist), and introduce tricks to automatically tune and schedule the learning rate for SGD (see this course's slides, https://arxiv.org/abs/1506.01186, and FastAI course for example http://fastai.org).\n",
    "\n",
    "## Fashion MNIST\n",
    "\n",
    "This 10 class dataset is a drop-in replacement for MNIST with clothes instead of digits. MNI is arguably overused in the ML community nowadays. It is subtancially harder to classify.\n",
    "\n",
    "![fashion_mnist](fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a few functions first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.cm import get_cmap\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some system/model hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "batch_size = 128\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 26337280/26421880 [00:20<00:00, 966666.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|          | 0/29515 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "32768it [00:00, 132178.39it/s]           \u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|          | 0/4422102 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 49152/4422102 [00:00<00:09, 471875.04it/s]\u001b[A\n",
      "  4%|▍         | 172032/4422102 [00:00<00:07, 574945.67it/s]\u001b[A\n",
      "  5%|▌         | 221184/4422102 [00:00<00:10, 411638.81it/s]\u001b[A\n",
      "  7%|▋         | 311296/4422102 [00:00<00:08, 485572.26it/s]\u001b[A\n",
      "  8%|▊         | 368640/4422102 [00:00<00:07, 507287.33it/s]\u001b[A\n",
      " 11%|█         | 483328/4422102 [00:00<00:06, 603537.45it/s]\u001b[A\n",
      " 13%|█▎        | 557056/4422102 [00:00<00:06, 619472.31it/s]\u001b[A\n",
      " 15%|█▍        | 647168/4422102 [00:01<00:08, 459831.06it/s]\u001b[A\n",
      " 20%|█▉        | 868352/4422102 [00:01<00:05, 600354.18it/s]\u001b[A\n",
      " 22%|██▏       | 974848/4422102 [00:01<00:05, 632662.06it/s]\u001b[A\n",
      " 24%|██▍       | 1081344/4422102 [00:01<00:04, 719156.85it/s]\u001b[A\n",
      " 27%|██▋       | 1179648/4422102 [00:01<00:04, 711415.39it/s]\u001b[A\n",
      " 29%|██▊       | 1269760/4422102 [00:01<00:04, 714041.00it/s]\u001b[A\n",
      " 31%|███▏      | 1384448/4422102 [00:01<00:03, 779116.15it/s]\u001b[A\n",
      " 35%|███▍      | 1540096/4422102 [00:02<00:03, 899182.51it/s]\u001b[A\n",
      " 39%|███▊      | 1703936/4422102 [00:02<00:02, 1035566.09it/s]\u001b[A\n",
      " 41%|████▏     | 1826816/4422102 [00:02<00:02, 1020309.28it/s]\u001b[A\n",
      " 44%|████▍     | 1941504/4422102 [00:02<00:02, 1042720.63it/s]\u001b[A\n",
      " 46%|████▋     | 2056192/4422102 [00:02<00:02, 1070980.04it/s]\u001b[A\n",
      " 50%|█████     | 2228224/4422102 [00:02<00:01, 1202127.74it/s]\u001b[A\n",
      " 53%|█████▎    | 2359296/4422102 [00:02<00:01, 1196452.32it/s]\u001b[A\n",
      " 57%|█████▋    | 2531328/4422102 [00:02<00:01, 1306816.77it/s]\u001b[A\n",
      " 62%|██████▏   | 2727936/4422102 [00:02<00:01, 1447188.87it/s]\u001b[A\n",
      " 67%|██████▋   | 2949120/4422102 [00:03<00:00, 1602739.90it/s]\u001b[A\n",
      " 71%|███████▏  | 3153920/4422102 [00:03<00:00, 1710686.43it/s]\u001b[A\n",
      " 76%|███████▌  | 3342336/4422102 [00:03<00:00, 1733351.34it/s]\u001b[A\n",
      " 80%|████████  | 3538944/4422102 [00:03<00:00, 1785438.18it/s]\u001b[A\n",
      " 84%|████████▍ | 3727360/4422102 [00:03<00:00, 1802009.50it/s]\u001b[A\n",
      " 89%|████████▊ | 3915776/4422102 [00:03<00:00, 1435942.64it/s]\u001b[A\n",
      " 92%|█████████▏| 4079616/4422102 [00:03<00:00, 1166964.60it/s]\u001b[A\n",
      " 95%|█████████▌| 4218880/4422102 [00:04<00:00, 1013137.66it/s]\u001b[A\n",
      " 98%|█████████▊| 4341760/4422102 [00:04<00:00, 900505.65it/s] \u001b[A\n",
      "4423680it [00:04, 1029166.53it/s]                            \u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5148 [00:00<?, ?it/s]\u001b[A\n",
      "8192it [00:00, 53159.48it/s]            \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26427392it [00:40, 966666.25it/s]                              "
     ]
    }
   ],
   "source": [
    "english_labels = [\"T-shirt/top\",\n",
    "                  \"Trouser\",\n",
    "                  \"Pullover\",\n",
    "                  \"Dress\",\n",
    "                  \"Coat\",\n",
    "                  \"Sandal\",\n",
    "                  \"Shirt\",\n",
    "                  \"Sneaker\",\n",
    "                  \"Bag\",\n",
    "                  \"Ankle boot\"]\n",
    "\n",
    "train_data = datasets.FashionMNIST('data', train=True, download=True,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                   ]))\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets's compute the average mean and std of the train images. We will\n",
    "use them for normalizing data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_seen = 0.\n",
    "mean = 0\n",
    "std = 0\n",
    "for train_batch, train_target in train_loader:\n",
    "    batch_size = train_batch.shape[0]\n",
    "    train_batch = train_batch.view(batch_size, -1)\n",
    "    this_mean = torch.mean(train_batch, dim=1)\n",
    "    this_std = torch.sqrt(\n",
    "        torch.mean((train_batch - this_mean[:, None]) ** 2, dim=1))\n",
    "    mean += torch.sum(this_mean, dim=0)\n",
    "    std += torch.sum(this_std, dim=0)\n",
    "    n_samples_seen += batch_size\n",
    "\n",
    "mean /= n_samples_seen\n",
    "std /= n_samples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2860) tensor(0.3202)\n"
     ]
    }
   ],
   "source": [
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now reload the data with a further `Normalize` transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST('data', train=True, download=False,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=mean.view(1),\n",
    "                                                            std=std.view(1))]))\n",
    "\n",
    "test_data = datasets.FashionMNIST('data', train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean.view(1),\n",
    "                                                           std=std.view(1))]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32,\n",
    "                                          shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a very simple model, suitable for CPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=(3, 3), padding=1)\n",
    "        self.dropout_2d = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 20, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout_2d(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = self.dropout_2d(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = x.view(-1, 7 * 7 * 20)  # flatten / reshape\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.fc1.reset_parameters()\n",
    "        self.fc2.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "- Define a VGG-like model: add more convolutional and max pooling layers to increase the number of channels progressively while decreasing the dimensions of the feature maps with max pooling.\n",
    "- Try to use Adam instead of SGD in conjunction with the `find_lr` heuristic and the cosine learning rate schedule below;\n",
    "- (optional) Try data augmentation (horizontal flips, random crops, cutout...);\n",
    "- (optional) Implement the [mixup stochastic label interpolation](https://arxiv.org/abs/1710.09412);\n",
    "- (optional) Try to use batch-normalization;\n",
    "- (optional) Implement skip-connections.\n",
    "\n",
    "See how you compare to other approaches:\n",
    "- https://github.com/zalandoresearch/fashion-mnist\n",
    "- https://www.kaggle.com/zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/vgg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: data\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=tensor([0.2860]), std=tensor([0.3202]))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "img, target = train_data[1]\n",
    "# n_channel, width, height\n",
    "print(img.shape)\n",
    "\n",
    "# First dimension should contain batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a training image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEWtJREFUeJzt3XuMXdV1x/Hfwp6xPUOwx6aYwTE4BQOyLHBgZIECJaUNEBQECImHEHIliBFKoJGCBKJ/lH+QUCFJkaginGJiVylJUWLgDwQBVAkiSoyxXewAtXHk4Bdj4wd+4werf8wBDTB37WHOPffc8f5+JMszd90zd8+Bn8+dWWfvbe4uAPk5ru4BAKgH4QcyRfiBTBF+IFOEH8gU4QcyRfiBTBF+IFOEH8jU2Fa+mJlxO+EIjB8/PqyfeuqpDWs7duwIj92/f39YT90BmqpPmDChYa2npyc89uDBg2G9v78/rB89ejSsH6vc3YbzvFLhN7MrJD0iaYykf3f3B8t8vTqZxeerztugZ8yYEdYfffTRhrWnnnoqPHbFihVh/dChQ2H98OHDYX327NkNa9dee2147Lp168L6Qw89FNZ37doV1nM34rf9ZjZG0r9J+q6kWZJuMrNZzRoYgGqV+Zl/rqT33P3P7n5I0q8lXd2cYQGoWpnwT5O0YdDnG4vHPsfM5pvZMjNbVuK1ADRZ5b/wc/cFkhZI/MIPaCdlrvybJE0f9PnXi8cAjAJlwv+GpJlm9g0z65R0o6RnmzMsAFWzMi0sM7tS0r9qoNW30N0fSDy/srf9dbbq5syZE9ZvvPHGsH7dddeF9VS/uru7u2Et6rNL0pQpU8J6ldasWRPWP/nkk7B+1llnhfXoPoAXXnghPPbhhx8O66tXrw7rdWpJn9/dn5P0XJmvAaAe3N4LZIrwA5ki/ECmCD+QKcIPZIrwA5kq1ef/yi/Wxrf3nnDCCWF98eLFDWvnnHNOeOxxx8X/xu7Zsyesp+a1R9NqU/cIdHR0hPWJEyeG9X379oX1qFdf9f970ToIqfsfOjs7w/qrr74a1m+55ZawXqXh9vm58gOZIvxApgg/kCnCD2SK8AOZIvxApmj1FV566aWwftpppzWsbd++PTw2NTV17Nh4cuWRI0fCemo6cyTVhkyt3jtmzJjKXrtKZaeA9/b2hvXLL788rL/77rthvQxafQBChB/IFOEHMkX4gUwRfiBThB/IFOEHMtXSLbrrdP7554f1qI8vSR9++GHDWqpPn+qFp7bgnjbtS7ugfU5XV1fDWqqXntplN/W9paYMR/301HTi1P0NqanQGzduHPHXTkl937fddltYv/vuu0u9fjNw5QcyRfiBTBF+IFOEH8gU4QcyRfiBTBF+IFNlt+heL2mPpKOSjrh7X+L5tc3nT/VV77rrrrAe9flT8/VTff5Uz/ixxx4L65s3b25Yi3rdknTKKaeE9S1btoT1MusBjBs3Ljz2+OOPD+vnnXdeWL/zzjsb1qL/nlL6/obUUu+p42fMmBHWy2jJFt2Fv3X3+EwCaDu87QcyVTb8Lun3Zvammc1vxoAAtEbZt/0XufsmMztJ0otm9q67vzL4CcU/CvzDALSZUld+d99U/L1V0hJJc4d4zgJ370v9MhBAa404/GbWbWZf+/RjSZdJWt2sgQGoVpm3/VMlLSmmbI6V9J/u/nxTRgWgctms2//666+H9ZNOOimsR3PHU2vbp/rVH330UVi/4IILwvpll13WsJZaC+CJJ54I67fffntYX706frMXbYWduv+hv78/rK9cuTKsr127tmEttRZAao2F1HoAZ599dlifPXt2w9qaNWvCY1NYtx9AiPADmSL8QKYIP5Apwg9kivADmcpm6e5zzz03rG/YsCGsR1NXU1NTU1LTQ1Oef77x7RX79u0Lj501a1ZYT02FXrJkSVi/6qqrGtZS016XL18e1lPLsUftuO7u7vDY1DTr1DTu999/P6xfeOGFDWtlW33DxZUfyBThBzJF+IFMEX4gU4QfyBThBzJF+IFMHTN9/miKpCRt27YtrKemaEbTT6NtqKV4Wqskbd++PaynRN/7xx9/HB7b29sb1h944IGwnvreoy3AU8dGvfDhiJY0T011LtvnP3DgQFi/+OKLG9YWLVoUHtssXPmBTBF+IFOEH8gU4QcyRfiBTBF+IFOEH8jUMdPnv+eee8J6qte+d+/esB71fVNf++DBg2E9dY9BX1+82dGUKVMa1iZPnhwe29HREdanTp0a1qM+vhR/752dneGxkyZNCus33HBDWO/p6WlYS/XhJ06cGNZTx6e+t9R/01bgyg9kivADmSL8QKYIP5Apwg9kivADmSL8QKaSfX4zWyjpe5K2uvvs4rHJkn4jaYak9ZKud/ed1Q0z7bXXXgvrJ598clg/44wzwnq0tn5qDfhoq2gpPXc8tb14NLc8Ne889dqpbbRTa+9Hc/ZTrx3tlSClt9mO1r/v6uoKj01936mxRWsJSNLTTz8d1lthOFf+X0q64guP3SvpZXefKenl4nMAo0gy/O7+iqQdX3j4akmfLjeySNI1TR4XgIqN9Gf+qe6+pfj4A0nxPaAA2k7pe/vd3c3MG9XNbL6k+WVfB0BzjfTK329mvZJU/L210RPdfYG797l7/TMZAHxmpOF/VtK84uN5kp5pznAAtEoy/Gb2pKT/kXSWmW00s1slPSjpO2a2VtLfF58DGEXMveGP681/seB3A3WL5n5L0syZMxvW7rjjjvDYSy65JKxv2LAhrKfmlu/atathLTVfP9XPrlJq3f5ULz21TkJ03latWhUee/PNN4f1dubu8YktcIcfkCnCD2SK8AOZIvxApgg/kCnCD2TqmFm6u6ydO+MZyUuXLm1YS22Dfemll4b1VLs1tQx0NKU41cpLTflNSbXronrqtceNGxfWDx06FNbHjx/fsJaaAp4DrvxApgg/kCnCD2SK8AOZIvxApgg/kCnCD2Qqmz5/qh+dmvoa9ZRTffrdu3eH9VQvPrXEdZlp2anz0sop319VmenI0TToZrx26h6GdjivXPmBTBF+IFOEH8gU4QcyRfiBTBF+IFOEH8hUNn3+VF/18OHDI/7a69atC+upPn9qm+vUvPVI6vuuus+f+vqR1PedujcjkvpvkpJaVjx1b0Y74MoPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECmkn1+M1so6XuStrr77OKx+yV9X9K24mn3uftzVQ2yFcr0bQ8cOBAem+pXp9anP3LkSFiP7hMo28cvsy6/FJ/X1Gun9kPo6uoK69HYUuc0B8O58v9S0hVDPP4zd59T/BnVwQdylAy/u78iaUcLxgKghcr8zP9DM3vLzBaaWU/TRgSgJUYa/p9LOl3SHElbJP2k0RPNbL6ZLTOzZSN8LQAVGFH43b3f3Y+6+yeSfiFpbvDcBe7e5+59Ix0kgOYbUfjNrHfQp9dKWt2c4QBoleG0+p6U9G1JJ5rZRkn/LOnbZjZHkktaL+n2CscIoALJ8Lv7TUM8/HgFY6lVmXnrqTXay667n6qn7lGIpMZeZm18Ke61p8ad+r5TYy9zj0FKO6y7XxZ3+AGZIvxApgg/kCnCD2SK8AOZIvxAprJZurtO06ZNC+s7d+4M66l2W9R2SrXTyiytXbXU2FPLrUffW9kW5rGAKz+QKcIPZIrwA5ki/ECmCD+QKcIPZIrwA5miz1+ocopm2WWiOzs7w3o0Zbjs0ttVLv2dmpKb2oI7tbR3NLYy23unvvZowZUfyBThBzJF+IFMEX4gU4QfyBThBzJF+IFM0edvgVQ/OjW3PHWfQHR8qpee6lenxpbafjz6+tHW4qljJWn//v1hPTJp0qQRH3us4MoPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECmkn1+M5suabGkqZJc0gJ3f8TMJkv6jaQZktZLut7d4wXoM5XqtZcVzZkvO++8ynX/y6wFMJzjo/sjJkyYEB6bkst8/iOSfuzusyRdIOkHZjZL0r2SXnb3mZJeLj4HMEokw+/uW9x9efHxHknvSJom6WpJi4qnLZJ0TVWDBNB8X+lnfjObIembkv4oaaq7bylKH2jgxwIAo8Sw7+03s+Ml/VbSj9x99+Cfx9zdzWzIH4LMbL6k+WUHCqC5hnXlN7MODQT/V+7+u+LhfjPrLeq9krYOday7L3D3Pnfva8aAATRHMvw2cIl/XNI77v7TQaVnJc0rPp4n6ZnmDw9AVYbztv9bkm6RtMrMVhaP3SfpQUn/ZWa3SvqLpOurGeLol2qXlVVl26nOVl/qtcu0+rq6usJjc5AMv7v/QVKj/8J/19zhAGgV7vADMkX4gUwRfiBThB/IFOEHMkX4gUyxdHehzimaqeWxyyg7bTalzNirnm4cbV1e5TkfLbjyA5ki/ECmCD+QKcIPZIrwA5ki/ECmCD+QKfr8hbLLREdS21hXObc8tWx42e3BqzxvZVXZ589l6W4AxyDCD2SK8AOZIvxApgg/kCnCD2SK8AOZos/fBsrMS5fiXnvqa5etp+4jqHNd/wjz+bnyA9ki/ECmCD+QKcIPZIrwA5ki/ECmCD+QqWSf38ymS1osaaokl7TA3R8xs/slfV/StuKp97n7c1UNtGpVzs/evHlzWD/zzDPDempOfdRrT/XhOzo6Rvy1h1OPzmvq/oWxY8vdhhK9NvP5h3eTzxFJP3b35Wb2NUlvmtmLRe1n7v5wdcMDUJVk+N19i6Qtxcd7zOwdSdOqHhiAan2ln/nNbIakb0r6Y/HQD83sLTNbaGY9DY6Zb2bLzGxZqZECaKphh9/Mjpf0W0k/cvfdkn4u6XRJczTwzuAnQx3n7gvcvc/d+5owXgBNMqzwm1mHBoL/K3f/nSS5e7+7H3X3TyT9QtLc6oYJoNmS4beBaVmPS3rH3X866PHeQU+7VtLq5g8PQFWG89v+b0m6RdIqM1tZPHafpJvMbI4G2n/rJd1eyQiPAZMmTQrr3d3dYT3V8jrxxBMb1spO2U21AstItfpS7bgNGzaE9WhJ9NNPPz08NqXsVOd2MJzf9v9B0lCTskdtTx8Ad/gB2SL8QKYIP5Apwg9kivADmSL8QKZYurtQ5VbTK1asCOtvv/12WN+1a1dYL9OLT/Wr9+7dG9ZT5yU6r2WmKkvprc97eoacbiJJWrp0aXhsymjo46dw5QcyRfiBTBF+IFOEH8gU4QcyRfiBTBF+IFPWyiWIzWybpL8MeuhESR+2bABfTbuOrV3HJTG2kWrm2E5z978azhNbGv4vvbjZsnZd269dx9au45IY20jVNTbe9gOZIvxApuoO/4KaXz/SrmNr13FJjG2kahlbrT/zA6hP3Vd+ADWpJfxmdoWZ/Z+ZvWdm99YxhkbMbL2ZrTKzlXVvMVZsg7bVzFYPemyymb1oZmuLvxvPW2392O43s03FuVtpZlfWNLbpZvbfZva2mf3JzP6xeLzWcxeMq5bz1vK3/WY2RtIaSd+RtFHSG5Jucvd4UnuLmNl6SX3uXntP2Mz+RtJeSYvdfXbx2L9I2uHuDxb/cPa4+z1tMrb7Je2te+fmYkOZ3sE7S0u6RtI/qMZzF4zretVw3uq48s+V9J67/9ndD0n6taSraxhH23P3VyTt+MLDV0taVHy8SAP/87Rcg7G1BXff4u7Li4/3SPp0Z+laz10wrlrUEf5pkgZvtbJR7bXlt0v6vZm9aWbz6x7MEKYW26ZL0geSptY5mCEkd25upS/sLN02524kO143G7/w+7KL3P08Sd+V9IPi7W1b8oGf2dqpXTOsnZtbZYidpT9T57kb6Y7XzVZH+DdJmj7o868Xj7UFd99U/L1V0hK13+7D/Z9uklr8vbXm8XymnXZuHmpnabXBuWunHa/rCP8bkmaa2TfMrFPSjZKerWEcX2Jm3cUvYmRm3ZIuU/vtPvyspHnFx/MkPVPjWD6nXXZubrSztGo+d22347W7t/yPpCs18Bv/dZL+qY4xNBjXX0v63+LPn+oem6QnNfA28LAGfjdyq6Qpkl6WtFbSS5Imt9HY/kPSKklvaSBovTWN7SINvKV/S9LK4s+VdZ+7YFy1nDfu8AMyxS/8gEwRfiBThB/IFOEHMkX4gUwRfiBThB/IFOEHMvX/wJIe16plA4kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(img[0].numpy(), cmap=get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension of the input data should contain the batch size (due to `torch.nn` API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(img[None, :])\n",
    "print(target, english_labels[target])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        batch_size = data.shape[0]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * batch_size\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_loss /= len(test_loader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a test function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            # sum up batch loss\n",
    "            _, pred = output.data.max(dim=1)\n",
    "            # get the index of the max log-probability\n",
    "            correct += torch.sum(pred == target.data.long()).item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = float(correct) / len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f},'\n",
    "              ' Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * test_accuracy))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_lr` function provides a learning rate for SGD or Adam, following heuristics from https://arxiv.org/abs/1506.01186:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loop_loader(data_loader):\n",
    "    while True:\n",
    "        for elem in data_loader:\n",
    "            yield elem\n",
    "\n",
    "def find_lr(model, train_loader, init_lr, max_lr, steps, n_batch_per_step=30):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=init_lr)\n",
    "    current_lr = init_lr\n",
    "    best_lr = current_lr\n",
    "    best_loss = float('inf')\n",
    "    lr_step = (max_lr - init_lr) / steps\n",
    "\n",
    "    loader = loop_loader(train_loader)\n",
    "    for i in range(steps):\n",
    "        mean_loss = 0\n",
    "        n_seen_samples = 0\n",
    "        for j, (data, target) in enumerate(loader):\n",
    "            if j > n_batch_per_step:\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            mean_loss += loss.item() * data.shape[0]\n",
    "            n_seen_samples += data.shape[0]\n",
    "            optimizer.step()\n",
    "\n",
    "        mean_loss /= n_seen_samples\n",
    "        print('Step %i, current LR: %f, loss %f' % (i, current_lr, mean_loss))\n",
    "            \n",
    "        if np.isnan(mean_loss) or mean_loss > best_loss * 4:\n",
    "            return best_lr / 4\n",
    "        \n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            best_lr = current_lr\n",
    "\n",
    "        current_lr += lr_step\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "\n",
    "    return best_lr / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our model on the GPU if required. We then define an optimizer and a learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 100\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.reset_parameters()\n",
    "lr = find_lr(model, train_loader, 1e-4, 1, 100, 30)\n",
    "model.reset_parameters()\n",
    "\n",
    "print('Best LR', lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                       T_max=3,\n",
    "                                                       last_epoch=-1)\n",
    "\n",
    "logs = {'epoch': [], 'train_loss': [], 'test_loss': [],\n",
    "        'test_accuracy': [], 'lr': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, optimizer, train_loader, epoch)\n",
    "    test_loss, test_accuracy = test(model, test_loader)\n",
    "    logs['epoch'].append(epoch)\n",
    "    logs['train_loss'].append(train_loss)\n",
    "    logs['test_loss'].append(test_loss)\n",
    "    logs['test_accuracy'].append(test_accuracy)\n",
    "    logs['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
