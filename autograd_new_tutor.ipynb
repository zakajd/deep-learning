{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch, Autograd и все-все-все\n",
    "Источники: \n",
    "- http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "- https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#conda install pytorch torchvision -c soumith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Tensor/Autograd\n",
    "============\n",
    "\n",
    "Простыми словами Tensor - это n-мерный массив в PyTorch. Особенность тензоров Pytorch состоит в том, что для них эффективно реализованы различные тензорные операции, с возможностью исполнения как на CPU, так и на GPU. В совопкупности с модулем __torch.autograd__ , они организуют высокопроизводительный интерфейс для построения графов вычислений с возможностью вычисления градиентов по узлам графа. Данный граф носит название - динамического графа вычислений(DCG), листья графа - входные тензора, корни - выходные. Градиенты считаются по пути от корней к листьям, при помощи chain rule.\n",
    "\n",
    "*Ранее был доп. класс torch.autorgrad.Variable для создания тензоров с поддержкой операций дифференцирования, но сейчас он deprecated*\n",
    "\n",
    "Параметры объекта __torch.Tensor__ используемые autorgrad'ом:\n",
    " - __.requires_grad__ = [True,False] нужно ли рассчитываеть градиенты для данного тензора. Возможно управление данным параметром через метод __.detach()__ - отключает дальнейшее вычисление градиентов для данного тензора. Так же можно использовать \"безградиентный контекст\" __with torch.no_grad(): ...__ \n",
    " - __.grad_fn__ - ссылка на функцию которая будет использована для вычисления градиентов\n",
    " - __.grad__ - значения градиента для данного тензора(считается на основе .grad_fn)\n",
    " - __.is_leaf__ - флаг, казывающий является ли данный тензор листом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6258,  0.3750],\n",
      "        [-0.7126, -0.7372]], requires_grad=True)\n",
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.randn(2, 2, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "# From numpy\n",
    "x = np.array([1., 2., 3.]) #Only Tensors of floating point dtype can require gradients\n",
    "x = torch.from_numpy(x)\n",
    "# Now enable gradient \n",
    "x.requires_grad_(True)\n",
    "print(x)\n",
    "# _ above makes the change in-place (its a common pytorch thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)  # we've created x ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем операции над x:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y был создан в результате операции сложения, поэтому у него должен быть grad_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f539c3157f0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще поупражняемся с y:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y is leaf : False\n",
      "Y req_grad : True\n",
      "Z is leaf : False\n",
      "Z req_grad : True\n",
      "X is leaf : True\n",
      "X req_grad : True\n"
     ]
    }
   ],
   "source": [
    "print(\"Y is leaf : {}\".format(y.is_leaf))\n",
    "print(\"Y req_grad : {}\".format(y.requires_grad))\n",
    "\n",
    "print(\"Z is leaf : {}\".format(z.is_leaf))\n",
    "print(\"Z req_grad : {}\".format(z.requires_grad))\n",
    "\n",
    "print(\"X is leaf : {}\".format(x.is_leaf))\n",
    "print(\"X req_grad : {}\".format(x.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты\n",
    "---------\n",
    "\n",
    "Давайте прогоним backprop и получим d(out)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример, когда в качестве корня графа мы получаем тензор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2)\n",
    "x.requires_grad_(True)\n",
    "y = x + 2\n",
    "y.backward(torch.ones(2, 2))\n",
    "# the retain_variables flag will prevent the internal buffers from being freed\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4022,  0.4708],\n",
      "        [ 0.1264,  1.7786]])\n",
      "tensor([[-1.8045, -0.0584],\n",
      "        [-0.7473,  2.5572]])\n",
      "tensor([[-3.2067, -0.5875],\n",
      "        [-1.6209,  3.3358]])\n"
     ]
    }
   ],
   "source": [
    "gradient = torch.randn(2, 2, requires_grad=False)\n",
    "\n",
    "# this would fail if we didn't specify\n",
    "# that we want to retain variables\n",
    "for i in range(3):\n",
    "    y.backward(gradient)\n",
    "    print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y is leaf : False\n",
      "Y req_grad : True\n",
      "X is leaf : True\n",
      "X req_grad : True\n"
     ]
    }
   ],
   "source": [
    "print(\"Y is leaf : {}\".format(y.is_leaf))\n",
    "print(\"Y req_grad : {}\".format(y.requires_grad))\n",
    "\n",
    "print(\"X is leaf : {}\".format(x.is_leaf))\n",
    "print(\"X req_grad : {}\".format(x.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
